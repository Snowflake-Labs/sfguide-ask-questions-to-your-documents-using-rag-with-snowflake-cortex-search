{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "gkhonmmpzyagn27drctl",
   "authorId": "5371469892340",
   "authorName": "CCARRERO",
   "authorEmail": "carlos.carrero@snowflake.com",
   "sessionId": "c4730a41-9dd6-4192-8883-ff75576e4a04",
   "lastEditTime": 1745407780247
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d285729c-a4fe-4d72-a997-b9749feeafa1",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "# Build a Retrieval Augmented Generation (RAG) based LLM assistant using Streamlit and Snowflake Cortex Search\n\n*NOTE: For prerequisites and other instructions, please refer to the [QuickStart Guide](https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/index.html#0).*"
  },
  {
   "cell_type": "markdown",
   "id": "eb88841d-dec0-4785-b730-11c337d33a48",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "## Setup\n\nCreate a database and a schema."
  },
  {
   "cell_type": "code",
   "id": "fed15752-0547-4bc4-982f-d1897ecd6072",
   "metadata": {
    "language": "sql",
    "name": "Create_DB",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--CREATE DATABASE If NOT EXISTS CC_QUICKSTART_CORTEX_SEARCH_DOCS;\n--CREATE SCHEMA If NOT EXISTS DATA;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22126130-bcda-458f-a683-48bdd5a59798",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## Organize Documents and Create Pre-Processing Function\n\nStep 1. Download sample [PDF documents](https://github.com/Snowflake-Labs/sfguide-ask-questions-to-your-documents-using-rag-with-snowflake-cortex-search/tree/main)."
  },
  {
   "cell_type": "markdown",
   "id": "6b2b9636-185d-4173-950a-4fffe04d9b24",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Step 2. Create a Stage with Directory Table where you will be uploading your documents."
  },
  {
   "cell_type": "code",
   "id": "8fca7c02-7e10-49e8-b46f-1091800b9bb7",
   "metadata": {
    "language": "sql",
    "name": "Create_STAGE",
    "collapsed": false
   },
   "outputs": [],
   "source": "create or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36d19eb0-604d-4c1a-99ec-2fb850465869",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "Step 3. Upload documents to your staging area\n\n- Select Data on the left\n- Click on your database CC_QUICKSTART_CORTEX_SEARCH_DOCS\n- Click on your schema DATA\n- Click on Stages and select DOCS\n- On the top right click on the **+Files** botton\n- Drag and drop the PDF documents you downloaded"
  },
  {
   "cell_type": "markdown",
   "id": "090e60b9-7165-46a8-b1dd-6a0ac0a2a88a",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "Step 4. Check files has been successfully uploaded"
  },
  {
   "cell_type": "code",
   "id": "982036d3-7162-4e27-bba7-4f4a13369e22",
   "metadata": {
    "language": "sql",
    "name": "ls_docs",
    "collapsed": false
   },
   "outputs": [],
   "source": "ls @docs;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc8317e0-97cb-4bee-8316-23adea17fb68",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "## Pre-process and Label Documents\n\nStep 1. Create the table where we are going to store the chunks for each PDF.\n\nWe are going to leverage Snowflake native document processing functions to prepare documents before enabling Cortex Search. We are also going to use Cortex CLASSIFY_TEXT function in order to label the type of document being processed so we can use that metadata to filter searches."
  },
  {
   "cell_type": "code",
   "id": "861fc5e2-f662-48e4-a481-baffc776f82a",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "CREATE or replace TEMPORARY table RAW_TEXT AS\nSELECT \n    RELATIVE_PATH,\n    SIZE,\n    FILE_URL,\n    build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n    TO_VARCHAR (\n        SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n            '@docs',\n            RELATIVE_PATH,\n            {'mode': 'LAYOUT'} ):content\n        ) AS EXTRACTED_LAYOUT \nFROM \n    DIRECTORY('@docs');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc188f35-e379-4dff-86f3-54a7355a3147",
   "metadata": {
    "language": "sql",
    "name": "create_table",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "create or replace TABLE DOCS_CHUNKS_TABLE ( \n    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n    SIZE NUMBER(38,0), -- Size of the PDF\n    FILE_URL VARCHAR(16777216), -- URL for the PDF\n    SCOPED_FILE_URL VARCHAR(16777216), -- Scoped url (you can choose which one to keep depending on your use case)\n    CHUNK VARCHAR(16777216), -- Piece of text\n    CHUNK_INDEX INTEGER, -- Index for the text\n    CATEGORY VARCHAR(16777216) -- Will hold the document category to enable filtering\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d2a50d6-4e45-474d-9761-7bb8d5055485",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "Step 2. Use the CORTREX PARSE_DOCUMENT function in order to read the PDF documents from the staging area. There is no need to create embeddings as that will be managed automatically by Cortex Search service later."
  },
  {
   "cell_type": "code",
   "id": "8102ebb4-08d4-4e27-a134-90c4d1d99aab",
   "metadata": {
    "language": "sql",
    "name": "insert_text_in_table",
    "collapsed": false
   },
   "outputs": [],
   "source": " insert into docs_chunks_table (relative_path, size, file_url,\n                            scoped_file_url, chunk, chunk_index)\n\n    select relative_path, \n            size,\n            file_url, \n            scoped_file_url,\n            c.value::TEXT as chunk,\n            c.INDEX::INTEGER as chunk_index\n            \n    from \n        raw_text,\n        LATERAL FLATTEN( input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n              EXTRACTED_LAYOUT,\n              'markdown',\n              1512,\n              256,\n              ['\\n\\n', '\\n', ' ', '']\n           )) c;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17c6b123-c948-413a-a166-632ab93ca60b",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "### Label the product category\n\nWe are going to use the power of Large Language Models and the function [CLASSIFY_TEXT](https://docs.snowflake.com/en/sql-reference/functions/classify_text-snowflake-cortex)  to easily classify the documents we are ingesting in our RAG application. We are going to pass the document name and the first chunk of text into the classify_text function.\n\nFirst we will create a temporary table with each unique file name and we will be passing that file name and the first chunk of text to CLASSIFY_TEXT. Classification is not mandatory for Cortex Search but we want to use it here to also demo hybrid search.\n\nRun this SQL to create that table:"
  },
  {
   "cell_type": "markdown",
   "id": "e04668d3-ace4-4fa7-8ede-847c57f8c6fb",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "1fc6189b-88fa-445a-aaff-c8c958b94c3e",
   "metadata": {
    "language": "sql",
    "name": "find_categories",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TEMPORARY TABLE docs_categories AS WITH unique_documents AS (\n  SELECT\n    DISTINCT relative_path, chunk\n  FROM\n    docs_chunks_table\n  WHERE \n    chunk_index = 0\n  ),\n docs_category_cte AS (\n  SELECT\n    relative_path,\n    TRIM(snowflake.cortex.CLASSIFY_TEXT (\n      'Title:' || relative_path || 'Content:' || chunk, ['Bike', 'Snow']\n     )['label'], '\"') AS category\n  FROM\n    unique_documents\n)\nSELECT\n  *\nFROM\n  docs_category_cte;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b3762b4-6338-4f22-aba2-6f29c494694b",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "You can check that table to identify how many categories have been created and if they are correct:"
  },
  {
   "cell_type": "code",
   "id": "203ffad6-1923-4418-8de2-043e7f2ede2e",
   "metadata": {
    "language": "sql",
    "name": "cell19",
    "collapsed": false
   },
   "outputs": [],
   "source": "select category from docs_categories group by category;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6fbfe778-a7d6-4224-b10f-8d9452d0d8b5",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "We can also check that each document category is correct:"
  },
  {
   "cell_type": "code",
   "id": "a9c2f9c4-28fc-40f1-b955-fe4bd8ab39fd",
   "metadata": {
    "language": "sql",
    "name": "cell21",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from docs_categories;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2cf54499-bc22-4524-a834-f676d0518181",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "Now we can just update the table with the chunks of text that will be used by Cortex Search service to include the category for each document:"
  },
  {
   "cell_type": "code",
   "id": "676bfda9-7fcd-41b0-99ae-6f6852c8651b",
   "metadata": {
    "language": "sql",
    "name": "update_categories",
    "collapsed": false
   },
   "outputs": [],
   "source": "update docs_chunks_table \n  SET category = docs_categories.category\n  from docs_categories\n  where  docs_chunks_table.relative_path = docs_categories.relative_path;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a184c76-1b33-435c-8ecf-a7afff307422",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": "## Create Cortex Search Service\n\nNext step is to create the CORTEX SEARCH SERVICE in the table we created before.\n\n- The name of the service is CC_SEARCH_SERVICE_CS.\n- The service will use the column chunk to create embeddings and perform retrieval based on similarity search.\n- The column category could be used as a filter.\n- To keep this service updated, warehosue COMPUTE_WH will be used. NOTE: You may replace the warehouse name with another one that you have access to.\n- The service will be refreshed every minute.\n- The data retrieved will contain the chunk, relative_path, file_url and category."
  },
  {
   "cell_type": "code",
   "id": "6971c036-f7b6-4ab3-935e-39095b44849d",
   "metadata": {
    "language": "sql",
    "name": "create_cortex_search_service",
    "collapsed": false
   },
   "outputs": [],
   "source": "create or replace CORTEX SEARCH SERVICE CC_SEARCH_SERVICE_CS\nON chunk\nATTRIBUTES category\nwarehouse = COMPUTE_WH\nTARGET_LAG = '1 minute'\nas (\n    select chunk,\n        chunk_index,\n        relative_path,\n        file_url,\n        category\n    from docs_chunks_table\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "afea4572-33e5-43fa-a94b-c7a2cc72c8b3",
   "metadata": {
    "name": "cell27",
    "collapsed": false
   },
   "source": "## Build Chat Interface\n\nTo build and run chat interface in Streamlit, continue and complete the steps outlined in the [QuickStart Guide](https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/index.html#4).\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "09d9ec4f-9e62-4808-a39c-8bdaddf3c9bd",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "## Automatic Processing of New Documents\nMaintaining your RAG system up to date when new documents are added, deleted or updated can be tedious. Snowflake makes it very easy. In one side, Cortex Search is a self-managed service. We only need to add, delete or update rows in the table where Cortex Search Service has been enabled and automatically the service will update the indexes and create new embeddings based on the frequency defined during service creation.\n\nIn addition, we can use Snowflake features like Streams, Tasks and Stored Procedures to automatically process new PDF files as they are added into Snowflake. \n\nFirst we create two streams for the DOCS staging area. One is going to be used to process deletes and other to process inserts. The Streams captures the chages on the Directory Table used for the DOCS staging area. So we can track new updates and deletes:"
  },
  {
   "cell_type": "code",
   "id": "b9107a30-0a78-44da-90bd-4468116cfbda",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "create or replace stream insert_docs_stream on stage docs;\ncreate or replace stream delete_docs_stream on stage docs;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9fc21f32-90e6-4c69-aeac-fe69b82f846c",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "Second, we are going to define a Stored Procedure that process those streams to:\n\n- Delete from the docs_chunk_table the content for files that has been deleted from the stagin area, so they are no longer relevant\n- Parse new PDF documents that has been added into the staging area using PARSE_DOCUMENT\n- Chunk the new document into pieces using SPLIT_TEXT_RECURSIVE_CHARACTER\n- Classify the new documents and update the label (this step is optional just to show the part of the possible)\n\nCreate the Stored Procedure:"
  },
  {
   "cell_type": "code",
   "id": "3efc4022-5179-4635-8cd3-18a8d7875d3a",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "create or replace procedure insert_delete_docs_sp()\nRETURNS VARCHAR\nLANGUAGE SQL\nAS\n$$\nBEGIN\n\nDELETE FROM docs_chunks_table\n    USING delete_docs_stream\n    WHERE docs_chunks_table.RELATIVE_PATH = delete_docs_stream.RELATIVE_PATH\n    and delete_docs_stream.METADATA$ACTION = 'DELETE';\n\n\nCREATE or replace TEMPORARY table RAW_TEXT AS\n    SELECT \n        RELATIVE_PATH,\n        SIZE,\n        FILE_URL,\n        build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n        TO_VARCHAR (\n            SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n                '@docs',\n                RELATIVE_PATH,\n                {'mode': 'LAYOUT'} ):content\n            ) AS EXTRACTED_LAYOUT \n    FROM \n        insert_docs_stream\n    WHERE \n        METADATA$ACTION = 'INSERT';\n\n    -- Insert new docs chunks\n    insert into docs_chunks_table (relative_path, size, file_url,\n                            scoped_file_url, chunk, chunk_index)\n\n    select relative_path, \n            size,\n            file_url, \n            scoped_file_url,\n            c.value::TEXT as chunk,\n            c.INDEX::INTEGER as chunk_index\n            \n    from \n        RAW_TEXT,\n        LATERAL FLATTEN( input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n              EXTRACTED_LAYOUT,\n              'markdown',\n              1512,\n              256,\n              ['\\n\\n', '\\n', ' ', '']\n           )) c;\n\n    -- Classify the new documents\n\n    CREATE OR REPLACE TEMPORARY TABLE docs_categories AS \n    WITH unique_documents AS (\n      SELECT DISTINCT\n        d.relative_path, d.chunk\n      FROM\n        docs_chunks_table d\n      INNER JOIN\n        RAW_TEXT r\n        ON d.relative_path = r.relative_path\n      WHERE \n        d.chunk_index = 0\n    ),\n    docs_category_cte AS (\n      SELECT\n        relative_path,\n        TRIM(snowflake.cortex.CLASSIFY_TEXT (\n          'Title:' || relative_path || 'Content:' || chunk, ['Bike', 'Snow']\n        )['label'], '\"') AS category\n      FROM\n        unique_documents\n    )\n    SELECT\n      *\n    FROM\n      docs_category_cte;\n\n    -- Update cathegories\n\n    update docs_chunks_table \n        SET category = docs_categories.category\n        from docs_categories\n        where  docs_chunks_table.relative_path = docs_categories.relative_path;\n\nEND;\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e6147c0e-e7f0-4dec-a0f7-3012adadeb90",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "Now we can create a Task that every X minutes can check if there is new data in the stream and take an action. We are setting the schedule to 5 minutes so you can follow the execution, but fell free to reduce the time to 1 minute if needed. Consider what would be best for your app and how often new docs are updated.\n\nWe define:\n  - Where: This is going to be executed using warehouse **COMPUTE_WH**. Please name to your own Warehouse.\n  - When: Check every 5 minutes, and execute in the case of new records in the delete_docs_stream stream (we could also use the other stream)\n  - What to do: call the stored procedure insert_delete_docs_sp()\n\nExecute this code in your Snowflake worksheet to create the task:"
  },
  {
   "cell_type": "code",
   "id": "cbcfc89c-77c4-4acf-95b4-c947a31f63f6",
   "metadata": {
    "language": "sql",
    "name": "cell15"
   },
   "outputs": [],
   "source": "create or replace task insert_delete_docs_task\n    warehouse = COMPUTE_WH\n    schedule = '5 minute'\n    when system$stream_has_data('delete_docs_stream')\nas\n    call insert_delete_docs_sp();\n\n\nalter task  insert_delete_docs_task resume;",
   "execution_count": null
  }
 ]
}