{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d285729c-a4fe-4d72-a997-b9749feeafa1",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "# Build a Retrieval Augmented Generation (RAG) based LLM assistant using Streamlit and Snowflake Cortex Search\n\n*NOTE: For prerequisites and other instructions, please refer to the [QuickStart Guide](https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/index.html#0).*"
  },
  {
   "cell_type": "markdown",
   "id": "eb88841d-dec0-4785-b730-11c337d33a48",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "## Setup\n\nCreate a database and a schema."
  },
  {
   "cell_type": "code",
   "id": "fed15752-0547-4bc4-982f-d1897ecd6072",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE DATABASE If NOT EXISTS CC_QUICKSTART_CORTEX_SEARCH_DOCS;\nCREATE SCHEMA If NOT EXISTS DATA;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22126130-bcda-458f-a683-48bdd5a59798",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## Organize Documents and Create Pre-Processing Function\n\nStep 1. Download sample [PDF documents](https://github.com/Snowflake-Labs/sfguide-ask-questions-to-your-documents-using-rag-with-snowflake-cortex-search/tree/main).\n\nStep 2. Create a table function that will read the PDF documents and split them in chunks. We will be using the PyPDF2 and Langchain Python libraries to accomplish the necessary document processing tasks. Because as part of Snowpark Python these are available inside the integrated Anaconda repository, there are no manual installs or Python environment and dependency management required."
  },
  {
   "cell_type": "code",
   "id": "50beb326-16d5-4fa7-8d6d-129bb97637af",
   "metadata": {
    "language": "sql",
    "name": "cell1",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "create or replace function pdf_text_chunker(file_url string)\nreturns table (chunk varchar)\nlanguage python\nruntime_version = '3.9'\nhandler = 'pdf_text_chunker'\npackages = ('snowflake-snowpark-python','PyPDF2', 'langchain')\nas\n$$\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom snowflake.snowpark.files import SnowflakeFile\nimport PyPDF2, io\nimport logging\nimport pandas as pd\n\nclass pdf_text_chunker:\n\n    def read_pdf(self, file_url: str) -> str:\n    \n        logger = logging.getLogger(\"udf_logger\")\n        logger.info(f\"Opening file {file_url}\")\n    \n        with SnowflakeFile.open(file_url, 'rb') as f:\n            buffer = io.BytesIO(f.readall())\n            \n        reader = PyPDF2.PdfReader(buffer)   \n        text = \"\"\n        for page in reader.pages:\n            try:\n                text += page.extract_text().replace('\\n', ' ').replace('\\0', ' ')\n            except:\n                text = \"Unable to Extract\"\n                logger.warn(f\"Unable to extract from file {file_url}, page {page}\")\n        \n        return text\n\n    def process(self,file_url: str):\n\n        text = self.read_pdf(file_url)\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 1512, #Adjust this as you see fit\n            chunk_overlap  = 256, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len\n        )\n    \n        chunks = text_splitter.split_text(text)\n        df = pd.DataFrame(chunks, columns=['chunks'])\n        \n        yield from df.itertuples(index=False, name=None)\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b2b9636-185d-4173-950a-4fffe04d9b24",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Step 3. Create a Stage with Directory Table where you will be uploading your documents."
  },
  {
   "cell_type": "code",
   "id": "8fca7c02-7e10-49e8-b46f-1091800b9bb7",
   "metadata": {
    "language": "sql",
    "name": "cell8",
    "collapsed": false
   },
   "outputs": [],
   "source": "create or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36d19eb0-604d-4c1a-99ec-2fb850465869",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "Step 4. Upload documents to your staging area\n\n- Select Data on the left\n- Click on your database CC_QUICKSTART_CORTEX_SEARCH_DOCS\n- Click on your schema DATA\n- Click on Stages and select DOCS\n- On the top right click on the **+Files** botton\n- Drag and drop the PDF documents you downloaded"
  },
  {
   "cell_type": "markdown",
   "id": "090e60b9-7165-46a8-b1dd-6a0ac0a2a88a",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "Step 5. Check files has been successfully uploaded"
  },
  {
   "cell_type": "code",
   "id": "982036d3-7162-4e27-bba7-4f4a13369e22",
   "metadata": {
    "language": "sql",
    "name": "cell11",
    "collapsed": false
   },
   "outputs": [],
   "source": "ls @docs;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc8317e0-97cb-4bee-8316-23adea17fb68",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "## Pre-process and Label Documents\n\nStep 1. Create the table where we are going to store the chunks for each PDF."
  },
  {
   "cell_type": "code",
   "id": "cc188f35-e379-4dff-86f3-54a7355a3147",
   "metadata": {
    "language": "sql",
    "name": "cell13",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "create or replace TABLE DOCS_CHUNKS_TABLE ( \n    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n    SIZE NUMBER(38,0), -- Size of the PDF\n    FILE_URL VARCHAR(16777216), -- URL for the PDF\n    SCOPED_FILE_URL VARCHAR(16777216), -- Scoped url (you can choose which one to keep depending on your use case)\n    CHUNK VARCHAR(16777216), -- Piece of text\n    CATEGORY VARCHAR(16777216) -- Will hold the document category to enable filtering\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d2a50d6-4e45-474d-9761-7bb8d5055485",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "Step 2. Use the function previously created to process the PDF files and extract the chunks. There is no need to create embeddings as that will be managed automatically by Cortex Search service later."
  },
  {
   "cell_type": "code",
   "id": "8102ebb4-08d4-4e27-a134-90c4d1d99aab",
   "metadata": {
    "language": "sql",
    "name": "cell15",
    "collapsed": false
   },
   "outputs": [],
   "source": "insert into docs_chunks_table (relative_path, size, file_url,\n                            scoped_file_url, chunk)\n    select relative_path, \n            size,\n            file_url, \n            build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n            func.chunk as chunk\n    from \n        directory(@docs),\n        TABLE(pdf_text_chunker(build_scoped_file_url(@docs, relative_path))) as func;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17c6b123-c948-413a-a166-632ab93ca60b",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "### Label the product category\n\nWe are going to use the power of Large Language Models to easily classify the documents we are ingesting in our RAG application. We are just going to use the file name but you could also use some of the content of the doc itself. Depending on your use case you may want to use different approaches. We are going to use a foundation LLM but you could even fine-tune your own LLM for your use case.\n\nFirst we will create a temporary table with each unique file name and we will be passing that file name to one LLM using Cortex Complete function with a prompt to classify what that use guide refres too. The prompt will be as simple as this but you can try to customize it depending on your use case and documents. Classification is not mandatory for Cortex Search but we want to use it here to also demo hybrid search.\n\nThis will be the prompt where we are adding the file name `Given the name of the file between <file> and </file> determine if it is related to bikes or snow. Use only one word <file> ' || relative_path || '</file>'`"
  },
  {
   "cell_type": "markdown",
   "id": "e04668d3-ace4-4fa7-8ede-847c57f8c6fb",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "1fc6189b-88fa-445a-aaff-c8c958b94c3e",
   "metadata": {
    "language": "sql",
    "name": "cell2",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE\nOR REPLACE TEMPORARY TABLE docs_categories AS WITH unique_documents AS (\n  SELECT\n    DISTINCT relative_path\n  FROM\n    docs_chunks_table\n),\ndocs_category_cte AS (\n  SELECT\n    relative_path,\n    TRIM(snowflake.cortex.COMPLETE (\n      'llama3-70b',\n      'Given the name of the file between <file> and </file> determine if it is related to bikes or snow. Use only one word <file> ' || relative_path || '</file>'\n    ), '\\n') AS category\n  FROM\n    unique_documents\n)\nSELECT\n  *\nFROM\n  docs_category_cte;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b3762b4-6338-4f22-aba2-6f29c494694b",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "You can check that table to identify how many categories have been created and if they are correct:"
  },
  {
   "cell_type": "code",
   "id": "203ffad6-1923-4418-8de2-043e7f2ede2e",
   "metadata": {
    "language": "sql",
    "name": "cell19",
    "collapsed": false
   },
   "outputs": [],
   "source": "select category from docs_categories group by category;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6fbfe778-a7d6-4224-b10f-8d9452d0d8b5",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "We can also check that each document category is correct:"
  },
  {
   "cell_type": "code",
   "id": "a9c2f9c4-28fc-40f1-b955-fe4bd8ab39fd",
   "metadata": {
    "language": "sql",
    "name": "cell21",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from docs_categories;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2cf54499-bc22-4524-a834-f676d0518181",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "Now we can just update the table with the chunks of text that will be used by Cortex Search service to include the category for each document:"
  },
  {
   "cell_type": "code",
   "id": "676bfda9-7fcd-41b0-99ae-6f6852c8651b",
   "metadata": {
    "language": "sql",
    "name": "cell23",
    "collapsed": false
   },
   "outputs": [],
   "source": "update docs_chunks_table \n  SET category = docs_categories.category\n  from docs_categories\n  where  docs_chunks_table.relative_path = docs_categories.relative_path;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a184c76-1b33-435c-8ecf-a7afff307422",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": "## Create Cortex Search Service\n\nNext step is to create the CORTEX SEARCH SERVICE in the table we created before.\n\n- The name of the service is CC_SEARCH_SERVICE_CS.\n- The service will use the column chunk to create embeddings and perform retrieval based on similarity search.\n- The column category could be used as a filter.\n- To keep this service updated, warehosue COMPUTE_WH will be used. NOTE: You may replace the warehouse name with another one that you have access to.\n- The service will be refreshed every minute.\n- The data retrieved will contain the chunk, relative_path, file_url and category."
  },
  {
   "cell_type": "code",
   "id": "6971c036-f7b6-4ab3-935e-39095b44849d",
   "metadata": {
    "language": "sql",
    "name": "cell25",
    "collapsed": false
   },
   "outputs": [],
   "source": "create or replace CORTEX SEARCH SERVICE CC_SEARCH_SERVICE_CS\nON chunk\nATTRIBUTES category\nwarehouse = COMPUTE_WH\nTARGET_LAG = '1 minute'\nas (\n    select chunk,\n        relative_path,\n        file_url,\n        category\n    from docs_chunks_table\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "afea4572-33e5-43fa-a94b-c7a2cc72c8b3",
   "metadata": {
    "name": "cell27",
    "collapsed": false
   },
   "source": "## Build Chat Interface\n\nTo build and run chat interface in Streamlit, continue and complete the steps outlined in the [QuickStart Guide](https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/index.html#4).\n\n"
  }
 ]
}